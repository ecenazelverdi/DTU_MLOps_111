{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DTU MLOps 111: Semantic Segmentation for Drone Imagery","text":"<p>Welcome to the documentation for Team 111's MLOps project!</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>This project implements semantic segmentation for drone imagery using the nnU-Net architecture. The system identifies five different classes of objects from aerial urban scenes:</p> <ul> <li>\ud83d\udfea Obstacles - Buildings, structures, and physical barriers</li> <li>\ud83d\udfe6 Water - Rivers, lakes, and water bodies</li> <li>\ud83d\udfe9 Soft Surfaces - Grass, vegetation, and unpaved areas</li> <li>\ud83d\udfe5 Moving Objects - Vehicles, people, and dynamic obstacles</li> <li>\u2b1c\ufe0f Landing Zones - Safe areas for drone landing</li> </ul>"},{"location":"#purpose","title":"Purpose","text":"<p>The trained model enhances the safety of autonomous drone flights and landings in urban areas by accurately distinguishing different types of obstacles from safe landing zones.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>New to the project? Start here:</p> <ol> <li>Installation - Set up your development environment</li> <li>Environment Setup - Configure credentials and paths</li> <li>Quick Start - Run your first workflow</li> </ol>"},{"location":"#user-guide","title":"\ud83d\udcda User Guide","text":"<ul> <li>Commands Reference - Complete <code>tasks.py</code> command documentation</li> <li>Workflows - Step-by-step guides for data, training, and API</li> <li>Troubleshooting - Solutions to common issues</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u2705 nnU-Net Architecture - State-of-the-art medical imaging segmentation adapted for drones</li> <li>\u2705 MLOps Best Practices - DVC for data versioning, Docker for containerization</li> <li>\u2705 FastAPI Inference - Production-ready REST API for real-time predictions</li> <li>\u2705 High-Performance Serving - BentoML integration for model packaging and adaptive batching</li> <li>\u2705 Cloud Deployment - GCP Cloud Run deployment with data drift monitoring</li> <li>\u2705 Comprehensive Testing - Automated tests with coverage reporting</li> </ul>"},{"location":"#dataset","title":"Dataset","text":"<p>The project uses the Semantic Segmentation Drone Dataset containing 400 RGB drone images with pixel-level semantic labels.</p> <p>Original dataset from TU Graz, IVC</p>"},{"location":"#technology-stack","title":"Technology Stack","text":"<ul> <li>Framework: PyTorch with nnU-Net</li> <li>Package Manager: UV</li> <li>Version Control: Git + DVC</li> <li>API: FastAPI + Uvicorn &amp; BentoML</li> <li>Containers: Docker</li> <li>Cloud: Google Cloud Platform (GCS, Cloud Run)</li> <li>Monitoring: Weights &amp; Biases, Evidently</li> </ul>"},{"location":"#team-111","title":"Team 111","text":"<ul> <li>Ecenaz Elverdi (s252699@dtu.dk)</li> <li>Akin Mert G\u00fcm\u00fcs (s242508@dtu.dk)</li> <li>Elif Pulukcu (s252749@dtu.dk)</li> <li>Kerick Jon Walker (s252618@dtu.dk)</li> <li>Bruno Zorrila Medina Luna (s260015@dtu.dk)</li> </ul> <p>Ready to get started? Head to the Installation Guide!</p>"},{"location":"source/commands/","title":"Commands Reference","text":"<p>Complete reference for all <code>tasks.py</code> invoke commands available in the project.</p>"},{"location":"source/commands/#overview","title":"Overview","text":"<p>All project commands are defined in <code>tasks.py</code> and executed using the <code>invoke</code> library. You run them with:</p> <pre><code>uv run invoke &lt;command-name&gt; [options]\n</code></pre>"},{"location":"source/commands/#list-all-commands","title":"List All Commands","text":"<p>To see all available commands:</p> <pre><code>uv run invoke --list\n</code></pre>"},{"location":"source/commands/#data-commands","title":"Data Commands","text":""},{"location":"source/commands/#download-data","title":"download-data","text":"<p>Downloads the drone imagery dataset from Kaggle.</p> <pre><code>uv run invoke download-data\n</code></pre> <p>Prerequisites:</p> <ul> <li>Kaggle credentials configured in <code>.env</code></li> <li><code>KAGGLE_USERNAME</code> and <code>KAGGLE_KEY</code> environment variables set</li> </ul> <p>Output:</p> <ul> <li>Downloads dataset to <code>data/raw/classes_dataset/</code></li> <li>Creates directories automatically if they don't exist</li> </ul> <p>What it does:</p> <ul> <li>Uses Kaggle API to download the semantic segmentation drone dataset</li> <li>Extracts the archive</li> <li>Organizes images into <code>original_images/</code> and <code>label_images_semantic/</code></li> </ul>"},{"location":"source/commands/#export-data","title":"export-data","text":"<p>Converts the downloaded Kaggle dataset into nnU-Net format.</p> <pre><code>uv run invoke export-data\n</code></pre> <p>Prerequisites:</p> <ul> <li>Dataset already downloaded with <code>download-data</code></li> <li>nnU-Net environment variables set in <code>.env</code></li> </ul> <p>Output:</p> <ul> <li>Creates <code>nnUNet_raw/Dataset101_DroneSeg/</code></li> <li>Populates <code>imagesTr/</code> and <code>labelsTr/</code> directories</li> <li>Generates <code>dataset.json</code> metadata file</li> </ul> <p>What it does:</p> <ul> <li>Reads images from <code>data/raw/classes_dataset/</code></li> <li>Converts to nnU-Net naming convention (<code>&lt;id&gt;_0000.png</code>)</li> <li>Creates train/test splits</li> <li>Generates nnU-Net dataset configuration</li> </ul>"},{"location":"source/commands/#download-and-export-data","title":"download-and-export-data","text":"<p>Combines <code>download-data</code> and <code>export-data</code> into a single command.</p> <pre><code>uv run invoke download-and-export-data\n</code></pre> <p>Equivalent to:</p> <pre><code>uv run invoke download-data\nuv run invoke export-data\n</code></pre>"},{"location":"source/commands/#download-models","title":"download-models","text":"<p>Downloads pre-trained models and checkpoints from DVC (Data Version Control).</p> <pre><code>uv run invoke download-models\n</code></pre> <p>Prerequisites:</p> <ul> <li>DVC configured (comes with <code>uv sync</code>)</li> <li>Access to the GCS bucket (for team members)</li> <li>Google Cloud credentials set in <code>.env</code> (if required)</li> </ul> <p>Output:</p> <ul> <li>Downloads tracked files: <code>nnUNet_results.dvc</code>, <code>nnUNet_preprocessed.dvc</code>, <code>data.dvc</code></li> <li>Populates directories with versioned data and models</li> </ul> <p>What it does:</p> <ul> <li>Runs <code>dvc pull</code> to fetch all DVC-tracked resources</li> <li>Downloads model checkpoints from cloud storage</li> <li>Syncs preprocessed data if needed</li> </ul>"},{"location":"source/commands/#preprocessing-commands","title":"Preprocessing Commands","text":""},{"location":"source/commands/#preprocess","title":"preprocess","text":"<p>Runs nnU-Net preprocessing pipeline.</p> <pre><code>uv run invoke preprocess [--dataset-id DATASET_ID]\n</code></pre> <p>Options:</p> <ul> <li><code>--dataset-id</code> (default: <code>101</code>) - Dataset ID to preprocess</li> </ul> <p>Examples:</p> <pre><code># Preprocess default dataset (101)\nuv run invoke preprocess\n\n# Preprocess specific dataset\nuv run invoke preprocess --dataset-id 102\n</code></pre> <p>Prerequisites:</p> <ul> <li>Data exported to <code>nnUNet_raw/</code> with <code>export-data</code></li> <li>nnU-Net environment variables set</li> </ul> <p>Output:</p> <ul> <li>Creates <code>nnUNet_preprocessed/Dataset101_DroneSeg/</code></li> <li>Generates preprocessing plans</li> <li>Creates <code>gt_segmentations/</code> and <code>nnUNetPlans_2d/</code> directories</li> </ul> <p>What it does:</p> <ul> <li>Analyzes dataset statistics</li> <li>Generates preprocessing plans</li> <li>Preprocesses images for training</li> <li>Verifies dataset integrity</li> </ul>"},{"location":"source/commands/#training-commands","title":"Training Commands","text":""},{"location":"source/commands/#train","title":"train","text":"<p>Trains the nnU-Net model.</p> <pre><code>uv run invoke train [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--dataset-id</code> (default: <code>101</code>) - Dataset ID</li> <li><code>--fold</code> (default: <code>0</code>) - Cross-validation fold (0-4)</li> <li><code>--dim</code> (default: <code>2d</code>) - Model dimension (<code>2d</code> or <code>3d_fullres</code>)</li> <li><code>--device</code> (default: <code>auto</code>) - Device to use (<code>auto</code>, <code>cuda</code>, <code>mps</code>, <code>cpu</code>)</li> </ul> <p>Examples:</p> <pre><code># Train with defaults (2D, fold 0, auto-detect device)\nuv run invoke train\n\n# Train on GPU with CUDA\nuv run invoke train --device cuda\n\n# Train on Apple Silicon\nuv run invoke train --device mps\n\n# Train on CPU\nuv run invoke train --device cpu\n\n# Train specific fold\nuv run invoke train --fold 1\n\n# Train 3D model\nuv run invoke train --dim 3d_fullres\n\n# Combine options\nuv run invoke train --dataset-id 101 --fold 2 --dim 2d --device cuda\n</code></pre> <p>Prerequisites:</p> <ul> <li>Data preprocessed with <code>preprocess</code></li> <li>Sufficient disk space for checkpoints (~500MB)</li> <li>GPU recommended (training is slow on CPU)</li> </ul> <p>Output:</p> <ul> <li>Model checkpoints in <code>nnUNet_results/Dataset101_DroneSeg/nnUNetTrainer__nnUNetPlans__2d/fold_X/</code></li> <li>Training logs and metrics</li> <li>Can be monitored with Weights &amp; Biases if configured</li> </ul> <p>What it does:</p> <ul> <li>Initializes nnU-Net trainer</li> <li>Trains segmentation model</li> <li>Saves checkpoints periodically</li> <li>Logs metrics to console and W&amp;B</li> </ul> <p>Device Selection:</p> <ul> <li><code>auto</code> - Automatically selects CUDA &gt; MPS &gt; CPU</li> <li><code>cuda</code> - Use NVIDIA GPU (requires CUDA toolkit)</li> <li><code>mps</code> - Use Apple Metal Performance Shaders (M1/M2 Macs)</li> <li><code>cpu</code> - Use CPU only (slow, for testing)</li> </ul>"},{"location":"source/commands/#testing-commands","title":"Testing Commands","text":""},{"location":"source/commands/#test","title":"test","text":"<p>Runs the test suite with coverage reporting.</p> <pre><code>uv run invoke test\n</code></pre> <p>What it does:</p> <ul> <li>Runs all tests in <code>tests/</code> directory with pytest</li> <li>Generates coverage report</li> <li>Displays coverage statistics</li> </ul> <p>Output:</p> <pre><code>tests/integrationtests/test_apis.py ...\ntests/test_data.py ...\n\n---------- coverage: platform darwin, python 3.12.x -----------\nName                              Stmts   Miss  Cover\n-----------------------------------------------------\nsrc/dtu_mlops_111/__init__.py        0      0   100%\nsrc/dtu_mlops_111/api.py           120     10    92%\nsrc/dtu_mlops_111/data.py           85      5    94%\n-----------------------------------------------------\nTOTAL                               205     15    93%\n</code></pre> <p>Manual Testing:</p> <pre><code># Run specific test file (integration tests for API)\nuv run pytest tests/integrationtests/test_apis.py\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test\nuv run pytest tests/test_data.py::test_download_data\n\n# Run without coverage\nuv run pytest tests/\n</code></pre>"},{"location":"source/commands/#api-commands","title":"API Commands","text":""},{"location":"source/commands/#app","title":"app","text":"<p>Starts the FastAPI development server.</p> <pre><code>uv run invoke app\n</code></pre> <p>What it does:</p> <ul> <li>Starts Uvicorn server on <code>http://localhost:8000</code></li> <li>Enables auto-reload on code changes</li> <li>Serves API endpoints for prediction</li> </ul> <p>Output:</p> <pre><code>INFO:     Uvicorn running on http://127.0.0.1:8000\nINFO:     Application startup complete.\n</code></pre> <p>Available Endpoints:</p> <ul> <li><code>GET /</code> - Health check &amp; model status</li> <li><code>POST /predict/</code> - Image segmentation (single)</li> <li><code>POST /batch_predict/</code> - Image segmentation (batch)</li> <li><code>GET /drift/</code> - Data drift report</li> <li><code>GET /model_info/</code> - Model metadata</li> <li><code>GET /metrics</code> - Prometheus metrics</li> </ul> <p>Testing the API:</p> <pre><code># In another terminal\ncurl http://localhost:8000/\n\n# Make a prediction\ncurl --location 'http://localhost:8000/predict/' \\\n  --form 'data=@\"path/to/image.png\"'\n</code></pre>"},{"location":"source/commands/#start-manually-with-uvicorn","title":"Start manually with Uvicorn","text":"<pre><code>uv run uvicorn main:app --port 8000 --reload\n</code></pre>"},{"location":"source/commands/#bentoml-serving-alternative","title":"BentoML Serving (Alternative)","text":""},{"location":"source/commands/#bento-build","title":"bento-build","text":"<p>Builds the BentoML bundle containing model and code.</p> <pre><code>uv run invoke bento-build\n</code></pre>"},{"location":"source/commands/#bento-serve","title":"bento-serve","text":"<p>Serves the BentoML service locally with auto-reload.</p> <pre><code>uv run invoke bento-serve\n</code></pre> <p>Output:</p> <pre><code>INFO [cli] Starting 1 BentoServer instances..\nINFO [cli] Service \"drone-seg-service\" is versioned: ...\nINFO [cli] Serving at http://localhost:3000\n</code></pre>"},{"location":"source/commands/#docker-build-bento","title":"docker-build-bento","text":"<p>Builds the Docker image for BentoML serving.</p> <pre><code>uv run invoke docker-build-bento\n</code></pre>"},{"location":"source/commands/#docker-run-bento","title":"docker-run-bento","text":"<p>Runs the BentoML container.</p> <pre><code>uv run invoke docker-run-bento\n</code></pre>"},{"location":"source/commands/#docker-commands","title":"Docker Commands","text":""},{"location":"source/commands/#docker-build","title":"docker-build","text":"<p>Builds all project Docker images (train, inference, api).</p> <pre><code>uv run invoke docker-build [--progress PROGRESS]\n</code></pre> <p>What it does:</p> <ul> <li>Builds <code>train:latest</code></li> <li>Builds <code>inference:latest</code></li> <li>Builds <code>api:latest</code></li> <li>Builds <code>bento:latest</code></li> </ul>"},{"location":"source/commands/#individual-build-tasks","title":"Individual Build Tasks","text":"<ul> <li><code>uv run invoke docker-build-train</code> - Build training image</li> <li><code>uv run invoke docker-build-inference</code> - Build inference image</li> <li><code>uv run invoke docker-build-api</code> - Build API image</li> </ul>"},{"location":"source/commands/#docker-train","title":"docker-train","text":"<p>Runs the training pipeline inside a Docker container with all necessary volume mounts and environment variables.</p> <pre><code>uv run invoke docker-train\n</code></pre> <p>What it does:</p> <ul> <li>Runs <code>train:latest</code> with <code>--gpus all</code> and <code>--ipc=host</code></li> <li>Mounts local <code>data/</code>, <code>nnUNet_raw/</code>, <code>nnUNet_preprocessed/</code>, and <code>nnUNet_results/</code></li> <li>Loads environment variables from <code>.env</code></li> </ul>"},{"location":"source/commands/#docker-inference","title":"docker-inference","text":"<p>Runs the nnU-Net prediction pipeline inside a Docker container.</p> <pre><code>uv run invoke docker-inference\n</code></pre> <p>What it does:</p> <ul> <li>Runs <code>inference:latest</code> with volume mounts for <code>nnUNet_results/</code>, <code>images_raw/</code>, and <code>visualizations/</code></li> </ul>"},{"location":"source/commands/#docker-run-api","title":"docker-run-api","text":"<p>Runs the FastAPI inference server in a container.</p> <pre><code>uv run invoke docker-run-api\n</code></pre> <p>What it does:</p> <ul> <li>Runs <code>api:latest</code> on port 8080</li> <li>Automatically mounts Google Cloud ADC credentials if found locally</li> </ul>"},{"location":"source/commands/#documentation-commands","title":"Documentation Commands","text":""},{"location":"source/commands/#build-docs","title":"build-docs","text":"<p>Builds the MkDocs documentation site.</p> <pre><code>uv run invoke build-docs\n</code></pre> <p>Output:</p> <ul> <li>Generates static HTML site in <code>docs/build/</code></li> </ul> <p>What it does:</p> <ul> <li>Reads <code>docs/mkdocs.yaml</code> configuration</li> <li>Processes markdown files from <code>docs/source/</code></li> <li>Generates HTML with Material theme</li> <li>Creates search index</li> </ul>"},{"location":"source/commands/#serve-docs","title":"serve-docs","text":"<p>Serves the documentation locally with auto-reload.</p> <pre><code>uv run invoke serve-docs\n</code></pre> <p>Output:</p> <pre><code>INFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  Documentation built in 0.52 seconds\nINFO    -  [18:00:00] Serving on http://127.0.0.1:8000/\n</code></pre> <p>What it does:</p> <ul> <li>Builds documentation</li> <li>Starts local server at <code>http://127.0.0.1:8000</code></li> <li>Watches for file changes and auto-rebuilds</li> <li>Hot-reloads browser on changes</li> </ul> <p>Usage:</p> <ul> <li>Open <code>http://127.0.0.1:8000</code> in your browser</li> <li>Edit markdown files in <code>docs/source/</code></li> <li>See changes automatically reflected</li> </ul> <p>Press <code>Ctrl+C</code> to stop the server.</p>"},{"location":"source/commands/#command-chaining","title":"Command Chaining","text":"<p>You can chain multiple commands if needed:</p> <pre><code># Download, export, and preprocess in one go\nuv run invoke download-data &amp;&amp; \\\nuv run invoke export-data &amp;&amp; \\\nuv run invoke preprocess\n</code></pre>"},{"location":"source/commands/#next-steps","title":"Next Steps","text":"<ul> <li>Workflows - Learn how to combine these commands into complete workflows</li> <li>Troubleshooting - Solutions if commands fail</li> </ul>"},{"location":"source/environment/","title":"Environment Setup","text":"<p>After installing the project, you need to configure environment variables for various services and tools. This page guides you through setting up your <code>.env</code> file.</p>"},{"location":"source/environment/#overview","title":"Overview","text":"<p>The project uses a <code>.env</code> file to store sensitive credentials and configuration paths. This file is not tracked in Git for security reasons.</p>"},{"location":"source/environment/#step-1-create-your-env-file","title":"Step 1: Create Your .env File","text":"<p>Copy the example environment file:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Now open <code>.env</code> in your text editor. You'll need to fill in several values.</p>"},{"location":"source/environment/#step-2-configure-kaggle-credentials","title":"Step 2: Configure Kaggle Credentials","text":"<p>The project downloads data from Kaggle, so you need a Kaggle API key.</p>"},{"location":"source/environment/#get-your-kaggle-username","title":"Get Your Kaggle username","text":"<ol> <li>Go to kaggle.com and log in</li> <li>Click on your profile picture \u2192 Your Profile</li> <li>The username is displayed on top of your name.</li> </ol>"},{"location":"source/environment/#get-your-kaggle-api-key","title":"Get Your Kaggle API Key","text":"<ol> <li>Go to kaggle.com and log in</li> <li>Click on your profile picture \u2192 Settings</li> <li>Scroll down to API section</li> <li>Click Create New Token</li> <li>Copy the generated token.</li> </ol>"},{"location":"source/environment/#add-to-env","title":"Add to .env","text":"<p>Copy these values into your <code>.env</code> file:</p> <pre><code># In your .env file\nKAGGLE_USERNAME=\"your_username_here\"\nKAGGLE_KEY=\"your_api_key_here\"\n</code></pre> <p>Keep Credentials Private</p> <p>Never commit your <code>.env</code> file or share your API keys publicly!</p>"},{"location":"source/environment/#step-3-configure-weights-biases-optional","title":"Step 3: Configure Weights &amp; Biases (Optional)","text":"<p>Weights &amp; Biases is used for experiment tracking.</p>"},{"location":"source/environment/#get-your-wandb-api-key","title":"Get Your Wandb API Key","text":"<ol> <li>Go to wandb.ai and sign up/log in</li> <li>Go to Settings \u2192 API Keys</li> <li>Scroll to API Keys section and generate a new API key or copy your existing API key</li> </ol>"},{"location":"source/environment/#add-to-env_1","title":"Add to .env","text":"<pre><code># In your .env file\nWANDB_API_KEY=\"your_wandb_api_key_here\"\n</code></pre> <p>Skip if Not Using W&amp;B</p> <p>If you're not using Weights &amp; Biases for experiment tracking, you can leave this empty or comment it out.</p>"},{"location":"source/environment/#step-4-configure-nnu-net-paths","title":"Step 4: Configure nnU-Net Paths","text":"<p>nnU-Net requires three environment variables pointing to data directories. These should be absolute paths on your system.</p>"},{"location":"source/environment/#understanding-the-directories","title":"Understanding the Directories","text":"<ul> <li><code>nnUNet_raw</code> - Raw dataset in nnU-Net format</li> <li><code>nnUNet_preprocessed</code> - Preprocessed data ready for training</li> <li><code>nnUNet_results</code> - Model checkpoints and training outputs</li> </ul>"},{"location":"source/environment/#add-to-env_2","title":"Add to .env","text":"<p>There are two ways to define variables in your <code>.env</code> file, depending on how you load them.</p>"},{"location":"source/environment/#option-1-standard-usage-recommended","title":"Option 1: Standard Usage (Recommended)","text":"<p>If you use <code>python-dotenv</code> (which this project does) or Docker, you generally don't need <code>export</code>. This is the standard format:</p> <pre><code># In your .env file\nnnUNet_raw=\"/absolute/path/to/DTU_MLOps_111/nnUNet_raw\"\nnnUNet_preprocessed=\"/absolute/path/to/DTU_MLOps_111/nnUNet_preprocessed\"\nnnUNet_results=\"/absolute/path/to/DTU_MLOps_111/nnUNet_results\"\n</code></pre>"},{"location":"source/environment/#option-2-shell-loading-with-export","title":"Option 2: Shell Loading (With Export)","text":"<p>If you plan to load these variables directly into your shell using <code>source .env</code>, you might need the <code>export</code> keyword so they become available to child processes:</p> <pre><code># In your .env file\nexport nnUNet_raw=\"/absolute/path/to/DTU_MLOps_111/nnUNet_raw\"\nexport nnUNet_preprocessed=\"/absolute/path/to/DTU_MLOps_111/nnUNet_preprocessed\"\nexport nnUNet_results=\"/absolute/path/to/DTU_MLOps_111/nnUNet_results\"\n</code></pre> <p>Example for macOS/Linux</p> <p><code>bash     nnUNet_raw=\"/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_raw\"     nnUNet_preprocessed=\"/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_preprocessed\"     nnUNet_results=\"/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_results\"</code></p> <p>Example for Windows</p> <p><code>bash     nnUNet_raw=\"C:/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_raw\"     nnUNet_preprocessed=\"C:/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_preprocessed\"     nnUNet_results=\"C:/Users/yourname/Documents/Github/DTU_MLOps_111/nnUNet_results\"</code></p> <p>Directories Created Automatically</p> <p>These environment variables tell the code where to look for or create these directories. The directories themselves will be created by the data processing commands (e.g., <code>make_dataset</code> or <code>export_dataset</code>), but defining them here ensures the code knows the correct locations. You don't need to manually create the folders before running the commands.</p>"},{"location":"source/environment/#step-5-configure-cloud-storage-optional","title":"Step 5: Configure Cloud Storage (Optional)","text":"<p>Google Cloud Storage (GCS) is used for data versioning with DVC and storing data drift reports.</p>"},{"location":"source/environment/#set-up-bucket-name","title":"Set Up Bucket Name","text":"<p>Add your GCS bucket name to your <code>.env</code> file:</p> <pre><code># In your .env file\nBUCKET_NAME=\"your-gcs-bucket-name\"\n</code></pre>"},{"location":"source/environment/#set-up-authentication","title":"Set Up Authentication","text":"<p>If you're using GCS for data versioning with DVC, you need to set up credentials.</p> <ol> <li>Run the login command:     <pre><code>uv run gcloud auth application-default login\n</code></pre></li> <li>Follow the browser prompt to authenticate.</li> <li>Done! The project's automated tasks (e.g., <code>uv run invoke docker-run-api</code>) are pre-configured to find these credentials automatically. You don't need to add anything to your <code>.env</code> file for authentication.</li> </ol>"},{"location":"source/environment/#complete-env-example","title":"Complete .env Example","text":"<p>Here's what your complete <code>.env</code> file should look like:</p>"},{"location":"source/environment/#option-1-standard-usage-recommended-for-pythondocker","title":"Option 1: Standard Usage (Recommended for Python/Docker)","text":"<pre><code># Kaggle credentials\nKAGGLE_USERNAME=\"john_doe\"\nKAGGLE_KEY=\"abc123def456ghi789\"\n\n# Weights &amp; Biases API key\nWANDB_API_KEY=\"1234567890abcdef\"\n\n# nnU-Net environment variables (use absolute paths!)\nnnUNet_raw=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_raw\"\nnnUNet_preprocessed=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_preprocessed\"\nnnUNet_results=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_results\"\n\n# Google Cloud Storage bucket name\nBUCKET_NAME=\"my-mlops-bucket\"\n</code></pre> <p>Automatic Loading</p> <p>The project's Python code uses <code>python-dotenv</code> to automatically load <code>.env</code> when running tasks. You typically don't need to manually source the file when using <code>uv run invoke</code> commands.</p>"},{"location":"source/environment/#option-2-shell-loading-with-export_1","title":"Option 2: Shell Loading (With Export)","text":"<p>Use this if you manually source your <code>.env</code> file (<code>source .env</code>).</p> <pre><code># Kaggle credentials\nKAGGLE_USERNAME=\"john_doe\"\nKAGGLE_KEY=\"abc123def456ghi789\"\n\n# Weights &amp; Biases API key\nWANDB_API_KEY=\"1234567890abcdef\"\n\n# nnU-Net environment variables (use absolute paths!)\nexport nnUNet_raw=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_raw\"\nexport nnUNet_preprocessed=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_preprocessed\"\nexport nnUNet_results=\"/Users/john/Documents/Github/DTU_MLOps_111/nnUNet_results\"\n\n# Google Cloud Storage bucket name\nexport BUCKET_NAME=\"my-mlops-bucket\"\n</code></pre>"},{"location":"source/environment/#verification","title":"Verification","text":""},{"location":"source/environment/#1-verify-standard-usage","title":"1. Verify Standard Usage","text":"<p>If you used Option 1, the variables are strictly for the Python application. Verify they work by running a project command:</p> <p>This command runs a Python script that loads .env and prints a variable</p> <pre><code>uv run python -c \"import os; from dotenv import load_dotenv; load_dotenv(); print(f\\\"nnUNet_raw: {os.environ.get('nnUNet_raw', 'NOT SET')}\\\")\"\n</code></pre> <p>Expected output:</p> <pre><code>nnUNet_raw: /Users/john/Documents/Github/DTU_MLOps_111/nnUNet_raw\n</code></pre> <p>If it says 'NOT SET', your .env file is not loading correcty</p>"},{"location":"source/environment/#2-verify-shell-loading","title":"2. Verify Shell Loading","text":"<p>If you used Option 2 (manual <code>export</code> or <code>source .env</code>), the variables are available in your terminal. You can check them directly:</p> <pre><code>echo $nnUNet_raw\necho $nnUNet_preprocessed\necho $nnUNet_results\n</code></pre>"},{"location":"source/environment/#next-steps","title":"Next Steps","text":"<p>With your environment configured, you're ready to:</p> <ol> <li>Quick Start - Run your first workflow</li> <li>Commands Reference - Learn about available commands</li> <li>Workflows - Step-by-step guides</li> </ol>"},{"location":"source/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"source/environment/#environment-variables-not-set","title":"Environment Variables Not Set","text":"<p>If you get errors about missing environment variables:</p> <pre><code># Make sure to source your .env file\nsource .env\n\n# Or on Windows, reload the variables\n</code></pre>"},{"location":"source/environment/#permission-denied-on-nnu-net-directories","title":"Permission Denied on nnU-Net Directories","text":"<p>Make sure the paths exist and you have write permissions:</p> <pre><code>mkdir -p nnUNet_raw nnUNet_preprocessed nnUNet_results\nchmod 755 nnUNet_*\n</code></pre> <p>For more help, see Troubleshooting.</p>"},{"location":"source/installation/","title":"Installation Guide","text":"<p>This guide will walk you through setting up the DTU MLOps 111 project on your local machine.</p>"},{"location":"source/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.12 or higher - Check with <code>python --version</code></li> <li>Git - For cloning the repository</li> <li>UV Package Manager - Modern Python package and project manager (we'll install this below)</li> </ul> <p>Why UV?</p> <p>UV is a fast Python package manager written in Rust. It's significantly faster than pip and handles dependency resolution more efficiently. The project uses UV for all package management tasks.</p>"},{"location":"source/installation/#step-1-install-uv-package-manager","title":"Step 1: Install UV Package Manager","text":"<p>UV is the package manager used throughout this project. Install it based on your operating system:</p>"},{"location":"source/installation/#macos-and-linux","title":"macOS and Linux","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, restart your terminal or run:</p> <pre><code>source $HOME/.cargo/env\n</code></pre>"},{"location":"source/installation/#windows","title":"Windows","text":"<p>Using PowerShell:</p> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"source/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that UV is installed correctly:</p> <pre><code>uv --version\n</code></pre> <p>You should see output like: <code>uv 0.x.x</code></p> <p>UV Documentation</p> <p>For more details about UV, visit the official UV documentation.</p>"},{"location":"source/installation/#step-2-clone-the-repository","title":"Step 2: Clone the Repository","text":"<p>Clone the project repository from GitHub:</p> <pre><code>git clone https://github.com/ecenazelverdi/DTU_MLOps_111.git\ncd DTU_MLOps_111\n</code></pre>"},{"location":"source/installation/#step-3-install-project-dependencies","title":"Step 3: Install Project Dependencies","text":"<p>UV will automatically create a virtual environment and install all dependencies defined in <code>pyproject.toml</code>:</p> <pre><code>uv sync\n</code></pre> <p>This command will:</p> <ul> <li>Create a virtual environment in <code>.venv/</code></li> <li>Install all project dependencies</li> <li>Install development dependencies (pytest, ruff, pre-commit, etc.)</li> <li>Install the <code>dtu_mlops_111</code> package in editable mode</li> </ul> <p>First Installation</p> <p>The first <code>uv sync</code> may take a few minutes as it downloads and installs all dependencies, including PyTorch and nnU-Net.</p>"},{"location":"source/installation/#installing-pytorch","title":"Installing PyTorch","text":"<p>The project configuration includes indexes for both CPU and CUDA versions of PyTorch. <code>uv sync</code> will automatically install the appropriate version for your platform.</p> <p>For installation:</p> <pre><code>uv sync\n</code></pre>"},{"location":"source/installation/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>After installation, verify everything is set up correctly:</p> <pre><code># Activate the virtual environment (UV does this automatically, but you can do it manually)\nsource .venv/bin/activate  # On macOS/Linux\n# Or on Windows:\n# .venv\\Scripts\\activate\n\n# Check Python version\npython --version\n\n# Verify the package is installed\npython -c \"import dtu_mlops_111; print('Package installed successfully!')\"\n</code></pre>"},{"location":"source/installation/#step-5-install-dvc-data-version-control","title":"Step 5: Install DVC (Data Version Control)","text":"<p>DVC is used for data and model versioning. It should already be installed via <code>uv sync</code>, but verify:</p> <pre><code>uv run dvc --version\n</code></pre>"},{"location":"source/installation/#step-6-configure-git-optional-for-contributors","title":"Step 6: Configure Git (Optional for Contributors)","text":"<p>If you plan to contribute to the project, set up pre-commit hooks:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>This will automatically run code formatters and linters before each commit.</p>"},{"location":"source/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have the project installed, proceed to:</p> <ol> <li>Environment Setup - Configure your <code>.env</code> file with credentials and paths</li> <li>Quick Start - Run your first workflow</li> </ol>"},{"location":"source/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"source/installation/#uv-command-not-found","title":"UV Command Not Found","text":"<p>If <code>uv</code> is not recognized after installation:</p> <ul> <li>Restart your terminal</li> <li>Ensure the cargo bin directory is in your PATH: <code>export PATH=\"$HOME/.cargo/bin:$PATH\"</code></li> </ul>"},{"location":"source/installation/#python-version-issues","title":"Python Version Issues","text":"<p>If you have multiple Python versions:</p> <pre><code># Use UV with a specific Python version (e.g., 3.13)\nuv sync --python 3.13\n</code></pre>"},{"location":"source/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency resolution issues:</p> <pre><code># Clear the cache and reinstall\nuv cache clean\nuv sync --reinstall\n</code></pre> <p>For more issues, see the Troubleshooting page.</p>"},{"location":"source/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get up and running quickly with the most common workflows.</p>"},{"location":"source/quickstart/#using-the-application","title":"Using the Application","text":"<p>This guide focuses on the specific commands used to run the Drone Segmentation project. All commands are executed using <code>uv run invoke</code>, which automatically handles environment activation and dependencies.</p>"},{"location":"source/quickstart/#1-data-management","title":"1. Data Management","text":"<p>Commands for handling the dataset:</p> <pre><code># Download the dataset from Kaggle\nuv run invoke download-data\n\n# Convert downloaded data to nnU-Net format\nuv run invoke export-data\n\n# Run both steps (download + export)\nuv run invoke download-and-export-data\n</code></pre>"},{"location":"source/quickstart/#2-training-pipeline","title":"2. Training Pipeline","text":"<p>Commands to preprocess data and train models:</p> <pre><code># Preprocess data (required before training)\nuv run invoke preprocess\n\n# Train the model (automatically detects GPU/MPS/CPU)\nuv run invoke train\n</code></pre>"},{"location":"source/quickstart/#3-api-inference","title":"3. API &amp; Inference","text":"<p>Commands to serve the model and make predictions:</p> <pre><code># Start the FastAPI server locally\nuv run invoke app\n\n# The API will be available at http://localhost:8000\n</code></pre>"},{"location":"source/quickstart/#4-development-testing","title":"4. Development &amp; Testing","text":"<p>Commands for maintaining the codebase:</p> <pre><code># Run the test suite with coverage\nuv run invoke test\n</code></pre>"},{"location":"source/quickstart/#complete-workflow","title":"Complete Workflow","text":"<p>Here's the complete workflow from installation to running inference:</p>"},{"location":"source/quickstart/#1-initial-setup","title":"1. Initial Setup","text":"<pre><code># Clone and install\ngit clone https://github.com/ecenazelverdi/DTU_MLOps_111.git\ncd DTU_MLOps_111\nuv sync\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your credentials (see Environment Setup guide)\nsource .env\n</code></pre>"},{"location":"source/quickstart/#2-download-and-prepare-data","title":"2. Download and Prepare Data","text":"<pre><code># Download data from Kaggle\nuv run invoke download-data\n\n# Export to nnU-Net format\nuv run invoke export-data\n\n# Or do both at once\nuv run invoke download-and-export-data\n</code></pre>"},{"location":"source/quickstart/#3-preprocess-data","title":"3. Preprocess Data","text":"<pre><code># Preprocess with nnU-Net\nuv run invoke preprocess\n</code></pre> <p>This creates the <code>nnUNet_preprocessed/</code> directory with training-ready data.</p>"},{"location":"source/quickstart/#4-train-the-model","title":"4. Train the Model","text":"<pre><code># Train with default settings (auto-detect device)\nuv run invoke train\n\n# Train on specific device\nuv run invoke train --device cuda    # GPU with CUDA\nuv run invoke train --device mps     # Apple Silicon GPU\nuv run invoke train --device cpu     # CPU only\n</code></pre>"},{"location":"source/quickstart/#5-run-the-api","title":"5. Run the API","text":"<pre><code># Start the FastAPI server\nuv run invoke app\n\n# API will be available at http://localhost:8000\n</code></pre>"},{"location":"source/quickstart/#6-make-predictions","title":"6. Make Predictions","text":"<pre><code># Using curl\ncurl --location 'http://localhost:8000/predict/' \\\n  --form 'data=@\"path/to/your/image.png\"'\n\n# Or use the deployed API from our side\ncurl --location 'https://model-api-32512441443.europe-west1.run.app/predict/' \\\n  --form 'data=@\"path/to/your/image.png\"'\n</code></pre>"},{"location":"source/quickstart/#first-time-setup-checklist","title":"First-Time Setup Checklist","text":"<p>Use this checklist to ensure you've completed all setup steps:</p> <ul> <li>[ ] Install UV package manager</li> <li>[ ] Clone the repository</li> <li>[ ] Run <code>uv sync</code> to install dependencies</li> <li>[ ] Create <code>.env</code> file from <code>.env.example</code></li> <li>[ ] Add Kaggle credentials to <code>.env</code></li> <li>[ ] Add Weights &amp; Biases API key to <code>.env</code> (optional)</li> <li>[ ] Configure nnU-Net paths in <code>.env</code></li> <li>[ ] Load environment variables with <code>source .env</code></li> <li>[ ] Download data with <code>uv run invoke download-data</code></li> <li>[ ] Export data with <code>uv run invoke export-data</code></li> <li>[ ] Preprocess data with <code>uv run invoke preprocess</code></li> <li>[ ] Verify installation with <code>uv run invoke test</code></li> </ul>"},{"location":"source/quickstart/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"source/quickstart/#docker-workflow","title":"Docker Workflow","text":"<pre><code># Build both API and Training images\nuv run invoke docker-build\n\n# Run the API container locally (available at http://localhost:8080)\nuv run invoke docker-run-api\n</code></pre> <p>The training image built with <code>docker-build</code> is fully CUDA-compatible, allowing for high-performance training on NVIDIA GPUs.</p> <p>For detailed instructions on running the training container with GPU support and volume mounts, see the Detailed Docker Guide.</p>"},{"location":"source/quickstart/#quick-command-reference","title":"Quick Command Reference","text":"<p>Most common commands you'll use:</p> Command Description <code>uv sync</code> Install/update all dependencies <code>uv run invoke download-data</code> Download dataset from Kaggle <code>uv run invoke export-data</code> Convert data to nnU-Net format <code>uv run invoke preprocess</code> Preprocess data for training <code>uv run invoke train</code> Train the model <code>uv run invoke test</code> Run tests with coverage <code>uv run invoke app</code> Start the API server <code>uv run invoke build-docs</code> Build documentation <code>uv run invoke serve-docs</code> Serve docs locally <p>For a complete command reference, see Commands Reference.</p>"},{"location":"source/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with the basics:</p> <ul> <li>Commands Reference - Learn about all available commands and their options</li> <li>Workflows - Detailed step-by-step workflows for specific tasks</li> <li>Troubleshooting - Solutions to common issues</li> </ul>"},{"location":"source/quickstart/#getting-help","title":"Getting Help","text":"<p>If you run into issues:</p> <ol> <li>Check the Troubleshooting guide</li> <li>Review error messages carefully</li> <li>Check that your <code>.env</code> file is configured correctly</li> <li>Verify all dependencies are installed with <code>uv sync</code></li> <li>Contact the team members listed on the homepage</li> </ol>"},{"location":"source/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their solutions.</p>"},{"location":"source/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"source/troubleshooting/#uv-command-not-found","title":"UV Command Not Found","text":"<p>Problem: After installing UV, the command is not recognized.</p> <pre><code>uv: command not found\n</code></pre> <p>Solution:</p> <ol> <li>Restart your terminal</li> <li>Add cargo bin directory to PATH:    <pre><code>export PATH=\"$HOME/.cargo/bin:$PATH\"\n</code></pre></li> <li>Make it permanent by adding to your shell profile:    <pre><code>echo 'export PATH=\"$HOME/.cargo/bin:$PATH\"' &gt;&gt; ~/.zshrc  # macOS\necho 'export PATH=\"$HOME/.cargo/bin:$PATH\"' &gt;&gt; ~/.bashrc  # Linux\nsource ~/.zshrc  # or ~/.bashrc\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#python-version-mismatch","title":"Python Version Mismatch","text":"<p>Problem: Wrong Python version installed.</p> <pre><code>error: Python 3.11 is not supported (requires Python 3.12+)\n</code></pre> <p>Solution:</p> <pre><code># Use specific Python version with UV\nuv sync --python 3.13\n</code></pre>"},{"location":"source/troubleshooting/#dependency-resolution-errors","title":"Dependency Resolution Errors","text":"<p>Problem: UV can't resolve dependencies.</p> <pre><code>error: Failed to resolve dependencies\n</code></pre> <p>Solution:</p> <pre><code># Clear UV cache\nuv cache clean\n\n# Reinstall dependencies\nuv sync --reinstall\n\n# If still failing, check pyproject.toml for conflicts\n</code></pre>"},{"location":"source/troubleshooting/#environment-configuration","title":"Environment Configuration","text":""},{"location":"source/troubleshooting/#environment-variables-not-set","title":"Environment Variables Not Set","text":"<p>Problem: nnU-Net can't find data directories.</p> <pre><code>RuntimeError: nnUNet_raw is not defined\n</code></pre> <p>Solution:</p> <ol> <li>Ensure <code>.env</code> file exists:    <pre><code>ls .env\n</code></pre></li> <li>Load environment variables:    <pre><code>source .env\n</code></pre></li> <li>Verify they're set:    <pre><code>echo $nnUNet_raw\necho $nnUNet_preprocessed\necho $nnUNet_results\n</code></pre></li> <li>If empty, check <code>.env</code> file has correct <code>export</code> syntax:    <pre><code>nnUNet_raw=\"/absolute/path/to/nnUNet_raw\"\n</code></pre>    or    <pre><code>export nnUNet_raw=\"/absolute/path/to/nnUNet_raw\"\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#kaggle-api-credentials-invalid","title":"Kaggle API Credentials Invalid","text":"<p>Problem: Can't download data from Kaggle.</p> <pre><code>401 Unauthorized: Invalid API credentials\n</code></pre> <p>Solution:</p> <ol> <li>Verify credentials in <code>.env</code>:    <pre><code>cat .env | grep KAGGLE\n</code></pre></li> <li>Check Kaggle API key is current:</li> <li>Go to kaggle.com/settings</li> <li>Create new token</li> <li>Update <code>.env</code> with new credentials</li> <li>Ensure no extra spaces or quotes:    <pre><code>KAGGLE_USERNAME=\"your_username\"\nKAGGLE_KEY=\"your_api_key\"\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#weights-biases-authentication","title":"Weights &amp; Biases Authentication","text":"<p>Problem: W&amp;B login fails.</p> <pre><code>wandb: ERROR authentication failed\n</code></pre> <p>Solution:</p> <pre><code># Set API key in .env\nWANDB_API_KEY=\"your_key_here\"\n\n# Or login manually\nuv run wandb login\n</code></pre>"},{"location":"source/troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"source/troubleshooting/#data-download-fails","title":"Data Download Fails","text":"<p>Problem: Download from Kaggle times out or fails.</p> <p>Solution:</p> <pre><code># Try again with verbose output\nuv run python -c \"import kaggle; kaggle.api.dataset_download_files(\n    'santurini/semantic-segmentation-drone-dataset',\n    path='data/raw',\n    unzip=True\n)\"\n\n# Check internet connection\nping kaggle.com\n\n# Check disk space\ndf -h\n</code></pre>"},{"location":"source/troubleshooting/#data-export-fails","title":"Data Export Fails","text":"<p>Problem: Converting to nnU-Net format fails.</p> <pre><code>FileNotFoundError: data/raw/classes_dataset not found\n</code></pre> <p>Solution:</p> <ol> <li>Verify data was downloaded:    <pre><code>ls data/raw/classes_dataset/classes_dataset/original_images/\n</code></pre></li> <li>If missing, re-download:    <pre><code>uv run invoke download-data\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#preprocessing-fails","title":"Preprocessing Fails","text":"<p>Problem: nnU-Net preprocessing crashes.</p> <pre><code>RuntimeError: Dataset verification failed\n</code></pre> <p>Solution:</p> <ol> <li>Check data integrity:    <pre><code>ls nnUNet_raw/Dataset101_DroneSeg/imagesTr/ | wc -l\n# Should show number of images\n</code></pre></li> <li>Verify environment variables are absolute paths:    <pre><code>echo $nnUNet_raw\n# Should be /full/path/to/nnUNet_raw, not ./nnUNet_raw\n</code></pre></li> <li>Re-export data:    <pre><code>rm -rf nnUNet_raw/Dataset101_DroneSeg\nuv run invoke export-data\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#python-313-compatibility-distutils-error","title":"Python 3.13 Compatibility / Distutils Error","text":"<p>Problem: Preprocessing can fail on Python 3.13 with errors related to <code>distutils</code> (e.g., <code>ModuleNotFoundError: No module named 'distutils'</code>). This is due to <code>distutils</code> being removed in recent Python versions. Note that this error may not occur for everyone, as it depends on your specific environment and how dependencies are resolved.</p> <p>Solution: Downgrade your environment to Python 3.12, which is the fully supported version for this project's dependencies.</p> <pre><code># Force UV to use Python 3.12\nuv python install 3.12\nuv venv --python 3.12\nuv sync\n\n# Then try preprocessing again\nuv run invoke preprocess\n</code></pre>"},{"location":"source/troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"source/troubleshooting/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Problem: GPU runs out of memory during training.</p> <pre><code>RuntimeError: CUDA out of memory\n</code></pre> <p>Solution:</p> <ol> <li>Reduce batch size (nnU-Net does this automatically, but you can force smaller batch):    <pre><code># Train on CPU if GPU is too small\nuv run invoke train --device cpu\n</code></pre></li> <li>Use smaller model dimension:    <pre><code>uv run invoke train --dim 2d  # Instead of 3d_fullres\n</code></pre></li> <li>Close other GPU-using applications</li> <li>Monitor GPU usage:    <pre><code>nvidia-smi\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#frozen-training-background-workers-stopped","title":"Frozen Training / Background Workers Stopped","text":"<p>Problem: Training hangs indefinitely or fails with an error like <code>DataLoader worker (pid X) is killed</code> or <code>Background workers stopped</code>. No explicit \"Out of Memory\" error is shown. This is often due to shared memory (shm) exhaustion or system resource leaks during long training runs.</p> <p>Solution:</p> <ol> <li>Clear System Cache: Often a system restart (as you found) clears the leaked resources.</li> <li>Reduce Workers: If it happens frequently, try reducing the number of data loader workers in your training configuration (though nnU-Net handles this, system-level pressure can still trigger it).</li> <li>Check Shared Memory: If running in Docker, ensure you have increased <code>--shm-size</code> (see Docker OOM section above).</li> <li>Monitor RAM: Ensure your system is not swapping heavily.</li> </ol>"},{"location":"source/troubleshooting/#docker-out-of-memory","title":"Docker Out of Memory","text":"<p>Problem: Training crashes inside Docker with a \"Killed\" message or generic memory errors, even if your GPU has enough memory. This usually happens because the Docker VM itself (on macOS or Windows) has allocated too little RAM.</p> <p>Solution: Increase the memory allocated to Docker Desktop.</p> <ol> <li>Open Docker Desktop Settings (gear icon).</li> <li>Go to Resources -&gt; Advanced.</li> <li>Increase Memory (we recommend at least 16GB for stable training).</li> <li>Increase Swap to 4GB or more.</li> <li>Click Apply &amp; Restart.</li> </ol> <p>For Linux / CLI Users:</p> <p>On Linux, Docker usually has access to all system memory unless limited. If you encounter issues, ensure you are providing enough shared memory (essential for multi-processing in nnU-Net) and avoiding hard limits.</p> <pre><code># Add these flags to your 'docker run' command if needed:\ndocker run --shm-size=8gb \\        # Increase shared memory (v. important)\n           --memory=16g \\          # Set a limit if host is unstable\n           --memory-swap=20g \\     # Total limit including swap\n           ...\n</code></pre>"},{"location":"source/troubleshooting/#device-not-found","title":"Device Not Found","text":"<p>Problem: Can't use GPU for training.</p> <pre><code>RuntimeError: No CUDA-capable device is detected\n</code></pre> <p>Solution:</p> <ol> <li>Check CUDA availability:    <pre><code>uv run python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre></li> <li>If False, train on CPU:    <pre><code>uv run invoke train --device cpu\n</code></pre></li> <li>On Mac with M chip, use MPS:    <pre><code>uv run invoke train --device mps\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#training-extremely-slow","title":"Training Extremely Slow","text":"<p>Problem: Training takes forever.</p> <p>Solution:</p> <ol> <li>Use GPU: <pre><code>uv run invoke train --device cuda  # or mps if using Mac with M chip\n</code></pre></li> <li>Check you have preprocessed data: <pre><code>ls nnUNet_preprocessed/Dataset101_DroneSeg/\n</code></pre></li> <li>Monitor system resources: <pre><code>htop  # or Activity Monitor on macOS\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#api-issues","title":"API Issues","text":""},{"location":"source/troubleshooting/#port-already-in-use","title":"Port Already in Use","text":"<p>Problem: Can't start API server.</p> <pre><code>ERROR: [Errno 48] Address already in use\n</code></pre> <p>Solution:</p> <pre><code># Find process using port 8000\nlsof -i :8000\n\n# Kill the process\nkill -9 &lt;PID&gt;\n\n# Restart it\nuv run invoke app\n</code></pre>"},{"location":"source/troubleshooting/#module-not-found-error","title":"Module Not Found Error","text":"<p>Problem: API or scripts refer to the project package but can't find it.</p> <pre><code>ModuleNotFoundError: No module named 'dtu_mlops_111'\n</code></pre> <p>Solution:</p> <ol> <li>Reinstall dependencies:    <pre><code>uv sync --reinstall\n</code></pre></li> <li>If it persists, install in editable mode manually:    <pre><code>uv pip install -e .\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#model-not-found","title":"Model Not Found","text":"<p>Problem: API can't find model checkpoint.</p> <pre><code>FileNotFoundError: nnUNet_results/Dataset101_DroneSeg/.../checkpoint_final.pth\n</code></pre> <p>Solution:</p> <ol> <li>Download pre-trained model:    <pre><code>uv run invoke download-models\n</code></pre></li> <li>Or train your own:    <pre><code>uv run invoke train\n</code></pre></li> <li>Verify checkpoint exists:    <pre><code>find nnUNet_results/ -name \"checkpoint*.pth\"\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#prediction-fails","title":"Prediction Fails","text":"<p>Problem: API returns error on prediction.</p> <pre><code>422 Unprocessable Entity\n</code></pre> <p>Solution:</p> <ol> <li>Check image format (should be PNG or JPG)</li> <li>Verify file is uploaded correctly in request:    <pre><code>curl -X POST \"http://localhost:8000/predict/\" \\\n  -F \"data=@/path/to/image.png\"\n</code></pre></li> <li>Check API logs for detailed error</li> <li>Run integration tests:    <pre><code>uv run pytest tests/integrationtests/test_apis.py\n</code></pre></li> <li>Test with a sample image from the dataset</li> </ol>"},{"location":"source/troubleshooting/#docker-issues","title":"Docker Issues","text":""},{"location":"source/troubleshooting/#permission-denied","title":"Permission Denied","text":"<p>Problem: Docker build or run fails with permission error.</p> <pre><code>permission denied while trying to connect to Docker daemon\n</code></pre> <p>Solution:</p> <pre><code># Add user to docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# Or run with sudo\nsudo docker build ...\n</code></pre>"},{"location":"source/troubleshooting/#gpu-not-available-in-docker","title":"GPU Not Available in Docker","text":"<p>Problem: Docker container can't access GPU.</p> <p>Solution:</p> <ol> <li>Install NVIDIA Container Toolkit:    <pre><code># Follow: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\n</code></pre></li> <li>Verify Docker can see GPU:    <pre><code>docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n</code></pre></li> <li>Ensure you use <code>--gpus all</code> flag:    <pre><code>docker run --gpus all ...\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#build-context-too-large","title":"Build Context Too Large","text":"<p>Problem: Docker build is very slow.</p> <pre><code>Sending build context to Docker daemon  5.234GB\n</code></pre> <p>Solution:</p> <ol> <li>Check <code>.dockerignore</code> file exists</li> <li>Add large directories to <code>.dockerignore</code>:    <pre><code>data/\n.venv/\nnnUNet_preprocessed/\nnnUNet_results/\nwandb/\n.git/\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#dvc-issues","title":"DVC Issues","text":""},{"location":"source/troubleshooting/#dvc-pull-fails","title":"DVC Pull Fails","text":"<p>Problem: Can't download data from DVC.</p> <pre><code>ERROR: failed to pull data from remote\n</code></pre> <p>Solution:</p> <ol> <li>Check you have access to GCS bucket (team members only)</li> <li>Verify you are logged into Google Cloud with Application Default Credentials (ADC):    <pre><code>uv run gcloud auth application-default login\n</code></pre></li> <li>Ensure the correct project is set:    <pre><code>gcloud config set project your-project-id\n</code></pre></li> <li>Force re-pull:    <pre><code>uv run dvc pull --force\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#dvc-push-fails","title":"DVC Push Fails","text":"<p>Problem: Can't push data to DVC remote.</p> <pre><code>ERROR: failed to push data to remote - permission denied\n</code></pre> <p>Solution:</p> <ol> <li>Check you have write permissions to GCS bucket.</li> <li>Re-authenticate with ADC:    <pre><code>uv run gcloud auth application-default login\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#testing-issues","title":"Testing Issues","text":""},{"location":"source/troubleshooting/#tests-fail","title":"Tests Fail","text":"<p>Problem: Pytest tests fail.</p> <p>Solution:</p> <ol> <li>Ensure dependencies are installed:    <pre><code>uv sync\n</code></pre></li> <li>Check test requirements:    <pre><code>uv run pytest tests/ -v\n</code></pre></li> <li>Run specific failing test with more output:    <pre><code>uv run pytest tests/integrationtests/test_apis.py::test_function -vv -s\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#import-errors-in-tests","title":"Import Errors in Tests","text":"<p>Problem: Tests can't import modules.</p> <pre><code>ModuleNotFoundError: No module named 'dtu_mlops_111'\n</code></pre> <p>Solution:</p> <ol> <li>Install package in editable mode:    <pre><code>uv sync\n</code></pre></li> <li>Verify pythonpath in <code>pyproject.toml</code>:    <pre><code>[tool.pytest.ini_options]\npythonpath = [\"src\"]\n</code></pre></li> </ol>"},{"location":"source/troubleshooting/#documentation-issues","title":"Documentation Issues","text":""},{"location":"source/troubleshooting/#mkdocs-build-fails","title":"MkDocs Build Fails","text":"<p>Problem: Documentation won't build.</p> <pre><code>Error: Config file 'mkdocs.yaml' does not exist\n</code></pre> <p>Solution:</p> <pre><code># Ensure you're running from project root\ncd /path/to/DTU_MLOps_111\n\n# Check config file exists\nls docs/mkdocs.yaml\n\n# Build with explicit config path\nuv run mkdocs build --config-file docs/mkdocs.yaml\n</code></pre>"},{"location":"source/troubleshooting/#missing-plugin","title":"Missing Plugin","text":"<p>Problem: MkDocs complains about missing plugin.</p> <pre><code>Error: Plugin 'mkdocstrings' not found\n</code></pre> <p>Solution:</p> <pre><code># Install mkdocs dependencies\nuv add --dev mkdocs-material mkdocstrings mkdocstrings-python\n\n# Or sync existing dependencies\nuv sync\n</code></pre>"},{"location":"source/troubleshooting/#general-tips","title":"General Tips","text":""},{"location":"source/troubleshooting/#check-logs","title":"Check Logs","text":"<p>Most tools provide verbose logging:</p> <pre><code># UV with debug output\nuv -v sync\n\n# Invoke with echo\nuv run invoke command --echo\n\n# Python with debug\nPYTHONVERBOSE=1 uv run python script.py\n</code></pre>"},{"location":"source/troubleshooting/#clean-start","title":"Clean Start","text":"<p>If all else fails, start fresh:</p> <pre><code># Remove virtual environment\nrm -rf .venv\n\n# Remove UV cache\nuv cache clean\n\n# Reinstall everything\nuv sync --reinstall\n\n# Reload environment\nsource .env\n</code></pre>"},{"location":"source/troubleshooting/#get-help","title":"Get Help","text":"<p>If you're still stuck:</p> <ol> <li>Check error messages carefully</li> <li>Search GitHub issues</li> <li>Check nnU-Net documentation: nnU-Net GitHub</li> <li>Contact team members (see homepage)</li> </ol>"},{"location":"source/troubleshooting/#still-having-issues","title":"Still Having Issues?","text":"<p>If your problem isn't listed here:</p> <ol> <li>Check the nnU-Net documentation</li> <li>Review error messages and stack traces</li> <li>Enable verbose/debug logging</li> <li>Contact the project team</li> </ol>"},{"location":"source/workflows/","title":"Workflows","text":"<p>Detailed step-by-step workflows for common tasks in the project.</p>"},{"location":"source/workflows/#data-workflow","title":"Data Workflow","text":"<p>Complete workflow from downloading data to having it ready for training.</p>"},{"location":"source/workflows/#step-1-download-dataset","title":"Step 1: Download Dataset","text":"<pre><code>uv run invoke download-data\n</code></pre> <p>What happens:</p> <ul> <li>Connects to Kaggle API</li> <li>Downloads semantic segmentation drone dataset</li> <li>Extracts to <code>data/raw/classes_dataset/</code></li> </ul> <p>Expected output:</p> <pre><code>data/raw/classes_dataset/classes_dataset/\n\u251c\u2500\u2500 original_images/        (400 RGB drone images)\n\u2514\u2500\u2500 label_images_semantic/  (400 segmentation masks)\n</code></pre>"},{"location":"source/workflows/#step-2-export-to-nnu-net-format","title":"Step 2: Export to nnU-Net Format","text":"<pre><code>uv run invoke export-data\n</code></pre> <p>What happens:</p> <ul> <li>Reads images from <code>data/raw/</code></li> <li>Converts to nnU-Net naming convention</li> <li>Creates dataset metadata</li> </ul> <p>Expected output:</p> <pre><code>nnUNet_raw/Dataset101_DroneSeg/\n\u251c\u2500\u2500 imagesTr/       (training images)\n\u251c\u2500\u2500 labelsTr/       (training labels)\n\u2514\u2500\u2500 dataset.json    (metadata)\n</code></pre>"},{"location":"source/workflows/#step-3-preprocess-data","title":"Step 3: Preprocess Data","text":"<pre><code>uv run invoke preprocess\n</code></pre> <p>What happens:</p> <ul> <li>Analyzes dataset statistics</li> <li>Generates preprocessing plans</li> <li>Creates training-ready data</li> </ul> <p>Expected output:</p> <pre><code>nnUNet_preprocessed/Dataset101_DroneSeg/\n\u251c\u2500\u2500 gt_segmentations/\n\u251c\u2500\u2500 nnUNetPlans_2d/\n\u2514\u2500\u2500 nnUNetPlans.json\n</code></pre> <p>Time: 10-30 minutes depending on your machine</p>"},{"location":"source/workflows/#training-workflow","title":"Training Workflow","text":"<p>End-to-end model training workflow.</p>"},{"location":"source/workflows/#prerequisites","title":"Prerequisites","text":"<ul> <li>[x] Data downloaded and preprocessed (see Data Workflow)</li> <li>[x] Environment variables configured</li> <li>[x] GPU available (recommended)</li> </ul>"},{"location":"source/workflows/#step-1-verify-data","title":"Step 1: Verify Data","text":"<pre><code># Check that preprocessed data exists\nls nnUNet_preprocessed/Dataset101_DroneSeg/\n\n# Should see:\n# gt_segmentations/  nnUNetPlans_2d/  nnUNetPlans.json\n</code></pre>"},{"location":"source/workflows/#step-2-start-training","title":"Step 2: Start Training","text":"<pre><code># Train with auto device detection\nuv run invoke train\n\n# Or specify device explicitly\nuv run invoke train --device cuda  # For NVIDIA GPU\nuv run invoke train --device mps   # For Apple Silicon\n</code></pre>"},{"location":"source/workflows/#step-3-monitor-training","title":"Step 3: Monitor Training","text":"<p>Training progress is logged to console:</p> <pre><code>Epoch 1/250\nCurrent loss: 0.4532\nValidation Dice: 0.7821\n</code></pre> <p>If you configured Weights &amp; Biases:</p> <ul> <li>Visit wandb.ai to see live metrics</li> <li>Charts for loss, Dice score, learning rate</li> <li>System metrics (GPU usage, memory)</li> </ul>"},{"location":"source/workflows/#step-4-verify-checkpoints","title":"Step 4: Verify Checkpoints","text":"<pre><code># Check training outputs\nls nnUNet_results/Dataset101_DroneSeg/nnUNetTrainer__nnUNetPlans__2d/fold_0/\n\n# Should contain:\n# checkpoint_final.pth\n# checkpoint_best.pth\n# training.log\n# progress.png\n</code></pre> <p>Training time:</p> <ul> <li>GPU: ~2-6 hours for full training</li> <li>CPU: ~24-48 hours (not recommended)</li> </ul>"},{"location":"source/workflows/#api-workflow","title":"API Workflow","text":"<p>Running and testing the inference API.</p>"},{"location":"source/workflows/#step-1-ensure-model-available","title":"Step 1: Ensure Model Available","text":"<p>You need a trained model checkpoint. Either:</p> <p>Option A: Use pre-trained model (recommended for testing)</p> <pre><code># Download from DVC\nuv run invoke download-models\n</code></pre> <p>Option B: Train your own model</p> <pre><code># Follow the Training Workflow above\nuv run invoke train\n</code></pre>"},{"location":"source/workflows/#step-2-start-api-server","title":"Step 2: Start API Server","text":"<pre><code>uv run invoke app\n</code></pre> <p>Output:</p> <pre><code>INFO:     Uvicorn running on http://127.0.0.1:8000\nINFO:     Application startup complete.\n</code></pre>"},{"location":"source/workflows/#step-3-test-api-health","title":"Step 3: Test API Health","text":"<p>In another terminal:</p> <pre><code># Check health endpoint (the root / behaves as health check)\ncurl http://localhost:8000/\n\n# Response: {\"status\": \"ok\", \"model_status\": \"model_loaded\"}\n</code></pre>"},{"location":"source/workflows/#step-4-make-a-prediction","title":"Step 4: Make a Prediction","text":"<pre><code># Predict on a sample image\ncurl --location 'http://localhost:8000/predict/' \\\n  --form 'data=@\"path/to/drone_image.png\"'\n</code></pre> <p>Response (example):</p> <pre><code>{\n  \"prediction_map_url\": \"gs://bucket/predictions/image_segmentation.png\",\n  \"class_percentages\": {\n    \"obstacles\": 32.4,\n    \"water\": 8.1,\n    \"soft_surfaces\": 25.3,\n    \"moving_objects\": 2.8,\n    \"landing_zones\": 31.4\n  }\n}\n</code></pre>"},{"location":"source/workflows/#step-5-view-interactive-documentation","title":"Step 5: View Interactive Documentation","text":"<p>Open in your browser:</p> <ul> <li>Swagger UI: <code>http://localhost:8000/docs</code></li> <li>ReDoc: <code>http://localhost:8000/redoc</code></li> </ul> <p>You can test all endpoints interactively from Swagger UI.</p>"},{"location":"source/workflows/#docker-workflow","title":"Docker Workflow","text":"<p>Using Docker for training and inference.</p>"},{"location":"source/workflows/#training-with-docker","title":"Training with Docker","text":""},{"location":"source/workflows/#step-1-build-training-container","title":"Step 1: Build Training Container","text":"<pre><code>uv run invoke docker-build-train\n</code></pre>"},{"location":"source/workflows/#step-2-run-training","title":"Step 2: Run Training","text":"<pre><code>uv run invoke docker-train\n</code></pre> <p>What it does:</p> <ul> <li>Runs the container with GPU support (<code>--gpus all</code>)</li> <li>Mounts local directories (<code>data/</code>, <code>nnUNet_raw/</code>, etc.) for persistence</li> <li>Executes the full training pipeline defined in <code>train_entrypoint.sh</code></li> </ul>"},{"location":"source/workflows/#api-with-docker","title":"API with Docker","text":""},{"location":"source/workflows/#step-1-build-api-container","title":"Step 1: Build API Container","text":"<pre><code>uv run invoke docker-build-api\n</code></pre>"},{"location":"source/workflows/#step-2-run-api","title":"Step 2: Run API","text":"<pre><code>uv run invoke docker-run-api\n</code></pre> <p>API available at: <code>http://localhost:8080</code></p>"},{"location":"source/workflows/#inference-with-docker","title":"Inference with Docker","text":""},{"location":"source/workflows/#step-1-build-inference-container","title":"Step 1: Build Inference Container","text":"<pre><code>uv run invoke docker-build-inference\n</code></pre>"},{"location":"source/workflows/#step-2-run-inference","title":"Step 2: Run Inference","text":"<pre><code>uv run invoke docker-inference\n</code></pre> <p>What it does:</p> <ul> <li>Runs <code>inference:latest</code> to generate masks from images in <code>images_raw/</code></li> <li>Saves results to <code>nnUNet_results/</code> and <code>visualizations/</code></li> </ul>"},{"location":"source/workflows/#bentoml-deployment-workflow","title":"BentoML Deployment Workflow","text":"<p>Alternative high-performance serving with BentoML. See BentoML Commands.</p>"},{"location":"source/workflows/#step-1-build-the-bento","title":"Step 1: Build the Bento","text":"<pre><code>uv run invoke bento-build\n</code></pre>"},{"location":"source/workflows/#step-2-serve-locally","title":"Step 2: Serve Locally","text":"<pre><code>uv run invoke bento-serve\n</code></pre>"},{"location":"source/workflows/#step-3-run-with-docker","title":"Step 3: Run with Docker","text":"<pre><code>uv run invoke docker-build-bento\nuv run invoke docker-run-bento\n</code></pre> <p>Quick start:</p> <pre><code># 1. Prepare input images\npython prepare_inference_input.py images_raw/ input/\n\n# 2. Build inference container\nuv run invoke docker_build_inference\n\n# 3. Run inference\nuv run invoke docker_inference\n\n# 4. Visualize results\npython visualize_results.py images_raw/ nnUNet_results/inference_outputs/ visualizations/\n</code></pre>"},{"location":"source/workflows/#testing-verification","title":"Testing &amp; Verification","text":"<p>Comprehensive testing to ensure project integrity.</p>"},{"location":"source/workflows/#1-unit-tests-coverage","title":"1. Unit Tests &amp; Coverage","text":"<pre><code># Run tests with coverage report\nuv run invoke test\n</code></pre>"},{"location":"source/workflows/#2-pipeline-verification-integration","title":"2. Pipeline Verification (Integration)","text":"<p>Verify the full data pipeline from download to preprocessing:</p> <pre><code>uv run invoke download-data &amp;&amp; uv run invoke export-data &amp;&amp; uv run invoke preprocess\n</code></pre>"},{"location":"source/workflows/#3-api-verification","title":"3. API Verification","text":"<pre><code># Run API integration tests\nuv run pytest tests/integrationtests/test_apis.py -v\n</code></pre>"},{"location":"source/workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Commands Reference - Detailed command documentation</li> <li>Troubleshooting - Solutions to common issues</li> </ul>"}]}